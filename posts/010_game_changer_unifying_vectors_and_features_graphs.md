---
title: 010_game_changer_unifying_vectors_and_features_graphs
layout: blog_default
date: 2025-11-12
categories: [arrowspace, graphs, vector space, analysis, MLops, AIops]
---

# Why `arrowspace` is game-changing for data operations at scale

TLDR; The possibility of having a vector database, graph-search capabilities and a key-value store in the same harness can make the difference for data practioners that need to face the challenge of datasets operations at AI-scale. Undecurrent: designing the data system for a planetary reserach vessel.

`arrowspace` v0.22.0 is out with improvements and a new graph motives API.

You can find `arrowspace` in the:

* [Rust repository](https://github.com/Mec-iS/arrowspace-rs) ↪️ `cargo add arrowspace`
* and [Python repository](https://github.com/tuned-org-uk/pyarrowspace) ↪️ `pip install arrowspace`

## Intro

`arrowspace` is a database for vectors supported by a graph representation and a key-value store. The main use-cases targeted are: **AI search capabilities as advanced vector similarity, graph characterisation analysis and search, indexing of high-dimensional vectors**.

## What ArrowSpace Is

`arrowspace` treats any dataset as a vector space with an added graph layered over vectors, enabling both geometric and topological operations in one index for search, matching, ranking, and dataset characterization at any lifecycle stage.
Two stabilised build paths—eigenmaps and pure energy maps—now provide spectral indices are provided that respect manifold structure instead of just geometric projection, with compact runtime footprints and query-time work limited to light scalar terms plus small projected features.

`arrowspace` taps from vectors, graphs, and key‑value stores paradigms by centering indexing on graph Laplacians constructed from data similarity graphs, aligning with standard spectral methods used to preserve manifold structure for search, matching, ranking, and characterization. This approach follows well‑documented pipelines in which a neighborhood graph over items is built from feature similarity, and a Laplacian operator provides the spectral coordinates and energies that capture both global topology and local geometry.


## Paradigm change 1. Any dataset is a graph

Vectors induce edges from item-space geometry while features induce topology; from these, `arrowspace` constructs a representative `GraphLaplacian` that supports reading, writing without index updates, matching, ranking, and cross-snapshot comparison of topology (global) and geometry (local) efficiently for evolving datasets.

A dataset can be cast as a similarity graph (ε‑graph, k‑NN, fully connected with kernels), enabling construction of a representative graph Laplacian whose eigenvectors and eigenvalues encode cluster structure and geometric relations for downstream tasks.[^1][^2]
Spectral views like Laplacian Eigenmaps and clustering leverage this Laplacian to embed data in coordinates faithful to the manifold while supporting comparisons across snapshots through stable spectral features.[^3][^2]
This Laplacian-centric view powers eigenmaps and energy maps so the index encodes structure directly, reducing dependence on cosine to tiebreaks when desired and enabling robust comparisons across datasets and time.

**Any vector space is automatically also a graph, for streamlining operations required by MLops and AIops teams in data-driven companies**. The geometric relations are defined from the items-space; the topological relations are defined from features-space. This is enough to generate a representative `GraphLaplacian` for any dataset, at any time of the lifecycle, in the most efficient way given the capabilities' constraints. The feature-space so generated by `arrowspace` works as a characterisation of the dataset in the sense that can be used to read (relatively fast for a prototype that includes topological structure), write (without index updating), match, rank, compare the dataset with any other snapshot of the dataset or other comparable datasets in terms of topology at wider scale and geometry at smaller scale.

## Paradigm change 2. High dimensions are the norm

Manifold methods such as Laplacian Eigenmaps and diffusion maps remain effective in high dimensions by exploiting locality and the connection of the graph Laplacian to the Laplace–Beltrami operator and heat equation, concentrating on geometry of the graph rather than raw ambient dimension.[^4][^3]
When needed, dimensionality can be reduced with probabilistic guarantees using the Johnson–Lindenstrauss lemma, preserving pairwise distances within $$1 \pm \epsilon$$ to keep search and analysis reliable at scale.[^6][^7]

Leveraging this approach on the features graph plus the other elements of energy dispersion and taumode make `arrowspace` well suited for datasets in which dimensionality is the main concern; allowing meaningful analysis and search even for datasets in which dimensions are overwhelmingly more than items presents in the dataset (think bag words with an 100k vocabulary but 10 millions of dimensions or a genome sequence with 20k genes but millions of potential relations among these genes or examples of prompts on a 70 millions weights model). **This paradigm can be scaled adaptively to lower-higher dimensions to deliver good results on any vector space used in research and commercial appllications**. This is inspired by full-disk and partial spectrography; for example analyzing Jupiter's entire disk produces aggregate albedo and phase-angle distributions, while partial observations of atmospheric regions yield localized composition signatures.Different spatial scales reveal different spectral characteristics, even within the same planetary system.

## Practical consequences

Algorithmic consequences:
- Sparse similarity graphs and a single sparse eigenproblem yield compact spectral artifacts and efficient pipelines, making it feasible to manage many dataset snapshots without heavy index churn.[^8][^2]
- Spectral embeddings let teams project items or segments into low‑dimensional coordinates for fast ranking, clustering, and cross‑dataset comparison grounded in graph geometry.[^4][^3]
- Graph‑induced embeddings and standard vector embeddings unify naturally by building the graph from vector similarities and then working in the same embedded space for retrieval and analysis.[^3][^2]
- A unified interface emerges because geometric proximity defines edges while topological structure is captured by the Laplacian spectrum, linking similarity, connectivity, and semantics in one operator.[^2][^3]

Consequences on a **practical level for data practioners** that are made possible by the `arrowspace` approach:
* **managing data archives** with >10k different datasets/snapshots with less infrastructural over-head via a local machine holding the space of the datasets while the datasets lives in their own enclaves
* do embeddings on anything, also on datasets themselves; turn any dataset into a single representative vector with F dimensions reliably
* make graph embeddings and non-graph embeddings to work together in the same space
* have a unified interface for the geometric and topological/semantic layers


### Who benefits

- Data science gains robust manifold‑aware embeddings and clusters that often outperform purely geometric clustering, with efficient implementations via standard linear algebra on sparse Laplacians.[^8][^2]
- Explainable and interpretable AI can attribute behavior to spectral coordinates and energies: the Rayleigh quotient of the Laplacian equals the normalized Dirichlet energy, making smoothness/roughness explicit and auditable.[^5][^9]
- MLOps and AIOps benefit from clear, reproducible choices of graph type, scaling, and normalization that materially affect outcomes—elements documented in the spectral clustering literature as key to stable results.[^2]
- Research using diffusion or wave‑like operators on graphs can leverage heat‑equation and wave‑equation formalisms on graph domains, enabling experiment designs that probe transport and interface effects.[^10][^4]

Briefly, these give unprecedented capabilities to people working in:
- **data science**: any vector space discovery is strengthed with spectral graph methods such as Laplacian Eigenmaps and the spectral clustering toolkit that translate similarity graphs into low‑dimensional, structure‑preserving coordinates for retrieval, clustering, and curation at scale.
- **explainable and interpretable AI**: any vector space is characterised for its properties allowing its connection to outcomes happening at inference time. Allowing data drifting control also on item-by-item addition bases thanks to taumode.
- **MLops and AIops**: Reproducible and auditable pipelines are supported by well‑documented sensitivities in spectral methods—graph type, scaling, normalization, and neighborhood size—all of which materially affect results and must be versioned alongside artifacts.
* edge research in in-memory processing and other fields that require simulating systems using dispersion/diffusion models (explain the difference between dispersion and diffusion, briefly explain how arrowspace uses dispersion, mention the possibility of simulating metamaterials with an extension of arrowspace to use complex values)


## Dispersion and diffusion

### Definition
Diffusion on graphs corresponds to applying heat‑like dynamics generated by the Laplacian, smoothing signals by iteratively reducing local differences along edges in a manner captured by diffusion maps and related Markov processes.[^4]
By contrast, dispersion (in the physical sense) denotes apparent spreading due to flow‑induced shear combined with molecular diffusion, as in Taylor–Aris dispersion, which enhances axial spreading even when molecular diffusivity is fixed.[^11][^12]

### How `arrowspace` uses diffusion-energy

In graph learning practice, diffusion is used to propagate local geometric information and reveal multiscale cluster structure, while energy terms such as Dirichlet energy and the Rayleigh quotient quantify smoothness or roughness that can be used to regularize rankings and embeddings.[^5][^4]
These spectral tools provide interpretable knobs that connect proximity, connectivity, and smoothness, supporting audit‑friendly retrieval and characterization beyond cosine‑only scoring.[^5][^2]

## Future developments

Algorithmic:
- Coherent, explainable diffusion‑based graph generation is a fast‑moving area, with discrete denoising diffusion models such as DiGress showing strong performance on molecular and non‑molecular graphs.[^13][^14][^15]
- Embedding causal relations into graph generation can build on differentiable causal discovery for DAG structure learning, where acyclicity constraints are enforced in continuous optimization (e.g., NOTEARS and its successors).[^16][^17][^18]
- Reinforcement learning can guide graph construction toward task rewards, as shown by Graph Convolutional Policy Networks that assemble molecular graphs under chemistry constraints while optimizing objectives.[^19][^20][^21]
- Complex‑valued or wave‑equation operators on graphs enable modeling interface phenomena, since wave dynamics have established graph formulations via edge‑based Laplacians and related operators.[^10]

### Potential future capabilities

- weighted features: provide masks to enable prioritising some features in the features space; for example prioritising position 0 and 1 as they hold longitude and latitude of a spatial feature vector — realizable via learned Mahalanobis metrics and local metric learning, or by coupling masks with anisotropic diffusion kernels to bias smoothing along salient coordinates.
- with enough accumulated data it will be probably possible to run coherent diffusion generation of graphs that will be explainable and controllable according to the source graphs — building on discrete denoising diffusion for graphs (e.g., DiGress) with spectral or topology-guided conditioning for controllability.
- Consider embedding causal relations in graph generation — inject acyclicity and intervention priors from differentiable causal discovery (e.g., NOTEARS and smooth acyclicity) to bias edge orientation and generative rollouts.
- use Reinforcement Learning for generating the graph as it is possible to learn how energy disperses along edges for class of datasets — leverage goal‑directed graph RL (e.g., GCPN) to learn edge additions guided by energy or spectral rewards aligned with downstream tasks.
- have a arrowspace-complex to work on complex numbers to allow Dirac-like dispersion for interfaces and boundaries simulation in novel materials — employ magnetic/complex Laplacians and graph wave dynamics to encode directionality and boundary conditions with phase information.
- anisotropic diffusion operators for direction‑ and feature‑aware smoothing — alternate closed‑form diffusion with local directional filters so that propagation respects flow, orientation, or masked features on the manifold.
- magnetic Laplacian modes for asymmetric or temporal graphs — use complex‑phase Laplacians to capture edge orientation and cyclic flows, improving embeddings and community detection in directed networks.
- topology‑aware feature masking via metric learning — learn global or local Mahalanobis metrics to turn masks into data‑driven edge weights that better reflect per‑domain importance before Laplacian construction.
- spectral positional encodings for sequences and spatiotemporal data — incorporate magnetic‑Laplacian‑based positional phases to preserve order and direction in spectral coordinates for retrieval and generation.
- causal‑aware diffusion priors — constrain diffusion steps or score fields with DAG structure to maintain identifiability of directions during graph synthesis and editing.

#### Notes on the name
`arrowspace` honors [Kenneth Arrow](https://en.wikipedia.org/wiki/Kenneth_Arrow)’s inquisitive, discussion-driven research ethos; "space" signals an added analytical layer that augments vector spaces with manifold-aware search and characterization.

#### References

- Ulrike von Luxburg, A Tutorial on Spectral Clustering (graph Laplacians, similarity graphs, algorithms, and practical choices).[^2]
- Belkin $$\&$$ Niyogi, Laplacian Eigenmaps (graph Laplacian, Laplace–Beltrami connection, manifold embeddings).[^8][^3]
- Coifman $$\&$$ Lafon, Diffusion Maps (Markov diffusion processes on graphs and multiscale geometry).[^4]
- Dirichlet energy and Rayleigh quotient on graphs (energy–smoothness link and spectral characterization).[^9][^5]
- Johnson–Lindenstrauss lemma (distance‑preserving dimension reduction).[^7][^6]
- Taylor–Aris dispersion vs diffusion (flow‑induced dispersion mechanisms).[^12][^11]
- Wave equations and edge‑based Laplacians on graphs (graph‑domain wave dynamics).[^10]
- DiGress: discrete diffusion for graph generation (graph generative diffusion).[^14][^15][^13]
- GCPN: reinforcement learning for graph construction (goal‑directed molecular graph generation).[^20][^21][^19]
- Differentiable causal discovery and DAG learning (acyclicity‑constrained structure learning).[^17][^18][^16]


[^1]: https://arxiv.org/abs/0711.0189 {#n01}

[^2]: https://people.csail.mit.edu/dsontag/courses/ml14/notes/Luxburg07_tutorial_spectral_clustering.pdf

[^3]: https://dl.acm.org/doi/10.1162/089976603321780317

[^4]: https://www.math.pku.edu.cn/teachers/yaoy/Fall2011/Lafon06.pdf

[^5]: https://arxiv.org/pdf/2203.03221.pdf

[^6]: https://www.math.toronto.edu/undergrad/projects-undergrad/Project03.pdf

[^7]: https://cs.stanford.edu/people/mmahoney/cs369m/Lectures/lecture1.pdf

[^8]: https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering

[^9]: https://fanchung.ucsd.edu/research/cb/ch1.pdf

[^10]: https://msp.org/pjm/2004/216-2/pjm-v216-n2-p03-p.pdf

[^11]: https://en.wikipedia.org/wiki/Taylor_dispersion

[^12]: https://www.azom.com/article.aspx?ArticleID=12173

[^13]: https://arxiv.org/abs/2209.14734

[^14]: https://arxiv.org/pdf/2209.14734.pdf

[^15]: https://iclr.cc/media/iclr-2023/Slides/11556.pdf

[^16]: https://openreview.net/pdf?id=IVwWgscehR

[^17]: https://papers.neurips.cc/paper/8157-dags-with-no-tears-continuous-optimization-for-structure-learning.pdf

[^18]: https://causaldm.github.io/Causal-Decision-Making/2_Causal_Structure_Learning/Causal%20Discovery.html

[^19]: http://papers.neurips.cc/paper/7877-graph-convolutional-policy-network-for-goal-directed-molecular-graph-generation.pdf

[^20]: https://dl.acm.org/doi/10.5555/3327345.3327537

[^21]: https://arxiv.org/abs/1806.02473

[^22]: https://github.com/cvignac/DiGress

[^23]: https://www.youtube.com/watch?v=k2saMtP-Fn8

[^24]: https://www.semanticscholar.org/paper/DiGress:-Discrete-Denoising-diffusion-for-graph-Vignac-Krawczuk/c65c4b68ac153176354a4c33c37f0ba1d86772c0

[^25]: https://www.ijcai.org/proceedings/2023/0751.pdf

[^26]: https://causalens.com/wp-content/uploads/2025/05/NOTEARS.pdf

[^27]: https://arxiv.org/pdf/1806.02473.pdf

[^28]: https://proceedings.neurips.cc/paper_files/paper/2024/file/91813e5ddd9658b99be4c532e274b49c-Paper-Conference.pdf

[^29]: https://www.semanticscholar.org/paper/Graph-Convolutional-Policy-Network-for-Molecular-You-Liu/d9f836a2062864e4808e12224e2286a353498202

[^30]: https://arxiv.org/pdf/2309.08406.pdf

[^31]: https://openreview.net/forum?id=vPx5855q0L

[^32]: https://www.alphaxiv.org/overview/2209.14734v4

[^33]: https://www.semanticscholar.org/paper/A-tutorial-on-spectral-clustering-Luxburg/eda90bd43f4256986688e525b45b833a3addab97

[^34]: https://cir.nii.ac.jp/crid/1571980075969154048

[^35]: https://www.academia.edu/27117748/A_tutorial_on_spectral_clustering

[^36]: https://www.scirp.org/reference/referencespapers

[^37]: https://www2.imm.dtu.dk/projects/manifold/Papers/Laplacian.pdf

[^38]: https://www.sciencedirect.com/science/article/pii/S1063520306000546

[^39]: https://www.bibsonomy.org/bibtex/ea91cbed60cf8e3dd610a1829afff18f

[^40]: https://www.semanticscholar.org/paper/Diffusion-maps-Coifman-Lafon/60290252155782642d39145421ba924e78263553

[^41]: https://www.semanticscholar.org/paper/Laplacian-Eigenmaps-for-Dimensionality-Reduction-Belkin-Niyogi/88816ae492956f3004daa41357166f1181c0c1bf

[^42]: https://arxiv.org/abs/math/0503445

[^43]: https://search.r-project.org/CRAN/refmans/Rdimtools/html/nonlinear_LAPEIG.html

[^44]: https://www.pnas.org/doi/10.1073/pnas.0500334102

[^45]: https://ieeexplore.ieee.org/document/6789755

[^46]: https://www.sciencedirect.com/science/article/pii/S1063520306000534

[^47]: https://www.authorea.com/users/176133/articles/213020/download_latex

[^48]: https://www.atascientific.com.au/wp-content/uploads/2017/02/WP150609UnderstandingTDA.pdf

[^49]: https://ibmm.umontpellier.fr/dsbc/tda_en.html

[^50]: https://pubs.rsc.org/en/content/articlelanding/2016/ay/c5ay02614h

[^51]: https://pmc.ncbi.nlm.nih.gov/articles/PMC3305791/

[^52]: https://optimization-online.org/wp-content/uploads/2015/07/4985.pdf

[^53]: https://nanoscale-metrix.com/ressources/taylor-dispersion-analysis-of-mixtures/

[^54]: https://math.umd.edu/courses/math420/2021S/LECTURES/DiscLec01.pdf

[^55]: https://arxiv.org/abs/1501.05877

[^56]: https://www.sciencedirect.com/science/article/pii/S0001870820300220

[^57]: https://courses.cs.duke.edu/fall15/compsci532/scribe_notes/lec20.pdf

[^58]: https://github.com/huggingface/awesome-papers/discussions/7

[^59]: https://www.techrxiv.org/doi/pdf/10.36227/techrxiv.175751921.18542359/v1

[^60]: https://borisburkov.net/2021-09-10-1/